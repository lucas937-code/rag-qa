{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65d123f",
   "metadata": {},
   "source": [
    "# RAG QA Evaluation\n",
    "\n",
    "This notebook can be used to evaluate either the RAG pipeline. It loads the prepared dataset and embedding, then executes the either the whole pipeline or just the retrieval and evaluates the results. \n",
    "\n",
    "## Colab Setup\n",
    "\n",
    "If you are running this notebook in Colab, we first need to clone the repository and install the requirements. Otherwise, we assume that you already have a clean setup to save execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    import os\n",
    "\n",
    "    REPO_URL = \"https://github.com/lucas937-code/rag-qa\"\n",
    "    REPO_DIR = \"rag-qa\"\n",
    "    BRANCH = \"dev\"\n",
    "    MODE = \"DEBUG\"\n",
    "\n",
    "    # Clone repo only if it does not exist yet\n",
    "    if not os.path.isdir(REPO_DIR):\n",
    "        print(f\"Cloning repository from {REPO_URL}...\")\n",
    "        !git clone {REPO_URL} {REPO_DIR}\n",
    "    else:\n",
    "        print(f\"Repository '{REPO_DIR}' already exists, skipping clone.\")\n",
    "\n",
    "    # Change into repo directory\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "    # Checkout the correct branch\n",
    "    if BRANCH != \"main\":\n",
    "        remote_branches = !git branch -r\n",
    "        print(remote_branches)\n",
    "        if f\"origin/{BRANCH}\" not in str(remote_branches):\n",
    "            raise ValueError(f\"Branch {BRANCH} does not exist\")\n",
    "        !git checkout {BRANCH}\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c212c",
   "metadata": {},
   "source": [
    "## Setup Configuration\n",
    "\n",
    "Initialize the configuration based on the execution environment (local using the HuggingFace API, local using an Ollama server or running in Colab using the HuggingFace API) and prepare the necessary directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eda398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import OllamaConfig, LocalConfig, ColabConfig\n",
    "\n",
    "USE_OLLAMA = True\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    OLLAMA_HOST = \"172.19.176.1\"\n",
    "    OLLAMA_PORT = 11434\n",
    "    OLLAMA_URL = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/chat\"\n",
    "    config = OllamaConfig(ollama_url=OLLAMA_URL)\n",
    "else:\n",
    "    config = ColabConfig() if IS_COLAB else LocalConfig()\n",
    "    \n",
    "config.ensure_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228d666",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation\n",
    "\n",
    "Retrieve chunks for a given number of samples and evaluate for each sample if at least one of the retrieved chunks contains the answer.  \n",
    "You can specify the number of samples, the number of candidates before the reranking is applied and a tuple of values for k that you want to evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate_retrieve import run_evaluation\n",
    "\n",
    "run_evaluation(config=config,\n",
    "               sample_limit=100,\n",
    "               candidates=100,\n",
    "               top_k=(1,3,5,10),\n",
    "               data_dirs=(config.train_dir, \n",
    "                          config.val_dir, \n",
    "                          config.test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877a50a",
   "metadata": {},
   "source": [
    "## Full RAG Evaluation\n",
    "\n",
    "Execute the whole RAG pipeline for a given number of samples and evaluate using two Benchmarks: \n",
    "**Exact Match (EM):** simple benchmark that indicates how often the LLM returned exactly the golden answer.\n",
    "\n",
    "**F1:** a more complex accuracy benchmark that calculates how similar the generated answer is to the golden answer on average. To do so, it uses the following steps:\n",
    "1. normalize prediction & golden answer:\n",
    "    - lowercasing\n",
    "    - remove punctuation\n",
    "    - remove \"a\"/\"an\"/\"the\"\n",
    "    - normalize white spaces\n",
    "2. tokenize prediction & golden answer\n",
    "3. count same tokens\n",
    "4. calculate precision & recall:\n",
    "    - $precision = \\frac{num_{same}}{|tokens_{prediction}|}$\n",
    "    - $recall = \\frac{num_{same}}{|tokens_{golden}|}$\n",
    "5. Calculate F1 score: $F1 = 2 \\cdot precision \\cdot \\frac{recall}{precision + recall}$ ($0$ if $num_{same}=0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate_rag_full import run_full_rag_eval\n",
    "\n",
    "run_full_rag_eval(config=config, \n",
    "                  max_questions=100, \n",
    "                  top_k=5,\n",
    "                  save_file=None)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
