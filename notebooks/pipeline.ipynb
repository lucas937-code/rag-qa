{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ffafb7",
   "metadata": {},
   "source": [
    "# RAG QA Pipeline\n",
    "\n",
    "This notebook demonstrates the complete Retrieval-Augmented Generation (RAG) pipeline for question answering. It loads the prepared data and embeddings, then executes queries to retrieve relevant context and generate answers.\n",
    "\n",
    "## Setup Configuration\n",
    "\n",
    "Initialize the configuration based on the execution environment (local using the HuggingFace API, local using an Ollama server or running in Colab using the HuggingFace API) and prepare the necessary directories.\n",
    "\n",
    "The following parameters can be adjusted:\n",
    "- `base_dir`: base directory where everything will be stored (it is recommended to use a mounted Google Drive directory if your are running in Colab, so the data can be stored persistently)\n",
    "- `hf_cache_dir`: cache that will be used by the HuggingFace library\n",
    "- `data_dir`: base directory where the dataset and embeddings will be stored (relative to `base_dir`)\n",
    "- `train_dir`: directory path where the training split will be stored (relative to `data_dir`)\n",
    "- `val_dir`: directory path where the validation split will be stored (relative to `data_dir`)\n",
    "- `test_dir`: directory path where the test split will be stored (relative to `data_dir`)\n",
    "- `embeddings_file`: file path where the pickle embeddings will be stored (relative to  `data_dir`; <span style=\"color:red;\">deprecated</span>)\n",
    "- `faiss_index_file`: file path where the faiss embeddings will be stored (relative to  `data_dir`)\n",
    "- `passages_file`: file path where the pickle file containing the passages will be stored (relative to  `data_dir`)\n",
    "- `embedding_model`: name of the embedding model to use\n",
    "- `rerank_model`: name of the reranker model to use\n",
    "- `generator_model`: name of the generator model to use (in case of an Ollama model, make sure it is installed)\n",
    "- `val_split_size`: size of the validation split (default is 7900, as specified in the assignment description)\n",
    "- `shard_batch_size`: number of samples that each shard contains (can be adjusted, depending on the available RAM)\n",
    "- `chunk_tokens`: \n",
    "- `chunk_overlap`: \n",
    "- `embeddings_batch_size`: "
   ]
  },
  {
   "cell_type": "code",
   "id": "3909df8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T22:08:10.546953Z",
     "start_time": "2025-12-06T22:08:10.541423Z"
    }
   },
   "source": [
    "from src.config import OllamaConfig, LocalConfig, ColabConfig, is_colab\n",
    "\n",
    "USE_OLLAMA = True\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    OLLAMA_HOST = \"172.19.176.1\"\n",
    "    OLLAMA_PORT = 11434\n",
    "    OLLAMA_URL = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/chat\"\n",
    "    config = OllamaConfig(ollama_url=OLLAMA_URL)\n",
    "else:\n",
    "    config = ColabConfig() if is_colab() else LocalConfig()\n",
    "    \n",
    "config.ensure_dirs()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/.hf_cache\n",
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/data\n",
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/data/train\n",
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/data/validation\n",
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/data/test\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "6d1695bf",
   "metadata": {},
   "source": [
    "## Load Embeddings\n",
    "\n",
    "Load the precomputed corpus and embeddings from the data preparation step."
   ]
  },
  {
   "cell_type": "code",
   "id": "3fafc462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T22:07:11.338202Z",
     "start_time": "2025-12-06T22:06:39.908770Z"
    }
   },
   "source": [
    "from src.load_data import load_embeddings\n",
    "\n",
    "corpus, emb = load_embeddings(config=config)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.virtualenvs/rag-qa/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loaded FAISS index with 978526 passages\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "68bf7fad",
   "metadata": {},
   "source": [
    "## Query and Generate Answer\n",
    "\n",
    "Execute a sample query through the RAG pipeline. The retriever fetches relevant context passages, and the generator produces an answer based on that context."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T22:11:28.548355Z",
     "start_time": "2025-12-06T22:11:28.545992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialize the retriever in an own cell, so you can\n",
    "from src.retriever import  Retriever\n",
    "\n",
    "retriever = Retriever()"
   ],
   "id": "8deea51f87ab1a22",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6efdc3fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T22:11:42.313454Z",
     "start_time": "2025-12-06T22:11:39.126871Z"
    }
   },
   "source": [
    "from src.generator import generate_answer_combined\n",
    "\n",
    "query = \"Who invented the speed of light?\"\n",
    "answer, ctx = generate_answer_combined(query, retriever, corpus, emb, config=config, top_k=5)\n",
    "\n",
    "print(\"\\nüîç Used Context Passages:\\n\")\n",
    "for i,p in enumerate(ctx,1):\n",
    "    print(f\"{i}. {p[:200].replace(chr(10),' ')}...\\n\")\n",
    "\n",
    "print(\"üí° Final Answer:\\n\", answer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Used Context Passages:\n",
      "\n",
      "1. Speed of light: snell's law using the opposing assumption, the more dense the medium the slower light traveled. fermat also argued in support of a finite speed of light. first measurement attempts in ...\n",
      "\n",
      "2. Speed of light: or eight minutes \" for the time taken for light to travel from the sun to the earth ( the modern value is 8 minutes 19 seconds ). newton queried whether r√∏mer's eclipse shadows were co...\n",
      "\n",
      "3. Light: the speed of light throughout history. galileo attempted to measure the speed of light in the seventeenth century. an early experiment to measure the speed of light was conducted by ole r√∏mer, ...\n",
      "\n",
      "4. Speed of light: a value of in 1862. in the year 1856, wilhelm eduard weber and rudolf kohlrausch measured the ratio of the electromagnetic and electrostatic units of charge, 1 / ‚àöŒµ0Œº0, by discharging ...\n",
      "\n",
      "5. Speed of light: ##r bodies. by the 14th century, sayana had made statements about the speed of light in his commentary on the hindu rigveda. in the early 17th century, johannes kepler believed that th...\n",
      "\n",
      "üí° Final Answer:\n",
      " \n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
