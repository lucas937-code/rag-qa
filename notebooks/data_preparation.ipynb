{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62655848",
   "metadata": {},
   "source": [
    "# RAG QA Data Preparation\n",
    "\n",
    "This notebook prepares and explores the dataset for the Retrieval-Augmented Generation (RAG) question-answering pipeline. It handles data loading, exploration, and embedding computation.\n",
    "\n",
    "## Setup Configuration\n",
    "\n",
    "Initialize the configuration based on the execution environment (local using the HuggingFace API, local using an Ollama server or running in Colab using the HuggingFace API) and prepare the necessary directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import OllamaConfig, LocalConfig, ColabConfig, is_colab\n",
    "\n",
    "USE_OLLAMA = True\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    OLLAMA_HOST = \"172.19.176.1\"\n",
    "    OLLAMA_PORT = 11434\n",
    "    OLLAMA_URL = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/chat\"\n",
    "    config = OllamaConfig(ollama_url=OLLAMA_URL)\n",
    "else:\n",
    "    config = ColabConfig() if is_colab() else LocalConfig()\n",
    "    \n",
    "config.ensure_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd2643",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Download and prepare the dataset using the configured data directory paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_data import ensure_data_available\n",
    "\n",
    "ensure_data_available(config=config)\n",
    "\n",
    "print(\"Dataset ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca5d23",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "\n",
    "Load and inspect samples from the training, validation, and test sets to understand the data structure and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explore_data import load_shards, explore_dataset\n",
    "\n",
    "train_ds = load_shards(config.train_dir, max_shards=3)\n",
    "val_ds   = load_shards(config.val_dir, max_shards=3)\n",
    "test_ds  = load_shards(config.test_dir, max_shards=3)\n",
    "\n",
    "explore_dataset(train_ds, \"Train set\")\n",
    "explore_dataset(val_ds, \"Validation set\")\n",
    "explore_dataset(test_ds, \"Test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146dc14",
   "metadata": {},
   "source": [
    "## Compute Embeddings\n",
    "\n",
    "Generate embeddings for the corpus using the configured model. These embeddings will be used for semantic similarity search in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.compute_embeddings import compute_embeddings\n",
    "\n",
    "corpus, corpus_embeddings = compute_embeddings(config=config)\n",
    "print(\"embeddings loaded\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
