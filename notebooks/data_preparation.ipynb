{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62655848",
   "metadata": {},
   "source": [
    "# RAG QA Data Preparation\n",
    "\n",
    "This notebook prepares and explores the dataset for the Retrieval-Augmented Generation (RAG) question-answering pipeline. It handles data loading, exploration, and embedding computation.\n",
    "\n",
    "## Colab Setup\n",
    "\n",
    "If you are running this notebook in Colab, we first need to clone the repository and install the requirements. Otherwise, we assume that you already have a clean setup to save execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbe32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    import os\n",
    "\n",
    "    REPO_URL = \"https://github.com/lucas937-code/rag-qa\"\n",
    "    REPO_DIR = \"rag-qa\"\n",
    "    BRANCH = \"dev\"\n",
    "    MODE = \"DEBUG\"\n",
    "\n",
    "    # Clone repo only if it does not exist yet\n",
    "    if not os.path.isdir(REPO_DIR):\n",
    "        print(f\"Cloning repository from {REPO_URL}...\")\n",
    "        !git clone {REPO_URL} {REPO_DIR}\n",
    "    else:\n",
    "        print(f\"Repository '{REPO_DIR}' already exists, skipping clone.\")\n",
    "\n",
    "    # Change into repo directory\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "    # Checkout the correct branch\n",
    "    if BRANCH != \"main\":\n",
    "        remote_branches = !git branch -r\n",
    "        print(remote_branches)\n",
    "        if f\"origin/{BRANCH}\" not in str(remote_branches):\n",
    "            raise ValueError(f\"Branch {BRANCH} does not exist\")\n",
    "        !git checkout {BRANCH}\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca9543",
   "metadata": {},
   "source": [
    "## Setup Configuration\n",
    "\n",
    "Initialize the configuration based on the execution environment (local using the HuggingFace API, local using an Ollama server or running in Colab using the HuggingFace API) and prepare the necessary directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import OllamaConfig, LocalConfig, ColabConfig\n",
    "\n",
    "USE_OLLAMA = not IS_COLAB and True     # change to False to disable Ollama in local setup\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    OLLAMA_HOST = \"172.19.176.1\"\n",
    "    OLLAMA_PORT = 11434\n",
    "    OLLAMA_URL = f\"http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/chat\"\n",
    "    config = OllamaConfig(ollama_url=OLLAMA_URL)\n",
    "else:\n",
    "    config = ColabConfig() if IS_COLAB else LocalConfig()\n",
    "    \n",
    "config.ensure_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd2643",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Download and prepare the dataset using the configured data directory paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_data import ensure_data_available\n",
    "\n",
    "ensure_data_available(config=config)\n",
    "\n",
    "print(\"Dataset ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca5d23",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "\n",
    "Load and inspect samples from the training, validation, and test sets to understand the data structure and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explore_data import load_shards, explore_dataset\n",
    "\n",
    "train_ds = load_shards(config.train_dir, max_shards=3)\n",
    "val_ds   = load_shards(config.val_dir, max_shards=3)\n",
    "test_ds  = load_shards(config.test_dir, max_shards=3)\n",
    "\n",
    "explore_dataset(train_ds, \"Train set\")\n",
    "explore_dataset(val_ds, \"Validation set\")\n",
    "explore_dataset(test_ds, \"Test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146dc14",
   "metadata": {},
   "source": [
    "## Compute Embeddings\n",
    "\n",
    "Generate embeddings for the corpus using the configured model. These embeddings will be used for semantic similarity search in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.compute_embeddings import compute_embeddings\n",
    "\n",
    "corpus, corpus_embeddings = compute_embeddings(config=config)\n",
    "print(\"embeddings loaded\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
