{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T09:32:53.779014Z",
     "start_time": "2025-11-30T09:32:53.762463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.config import LocalConfig, ColabConfig, is_colab\n",
    "\n",
    "config = ColabConfig(embedding_model=\"BAAI/bge-base-en\") if is_colab() else LocalConfig(embedding_model=\"BAAI/bge-base-en\")\n",
    "\n",
    "print(\"Using configuration:\", type(config).__name__)\n",
    "print(\"Base directory:\", config.BASE_DIR)\n",
    "\n",
    "config.ensure_dirs()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration: LocalConfig\n",
      "Base directory: /mnt/c/dev/ml/rag-qa\n",
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/.hf_cache\n",
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/data\n",
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/data/train\n",
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/data/validation\n",
      "‚úÖ Ensured directory exists: /mnt/c/dev/ml/rag-qa/data/test\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T09:32:58.928314Z",
     "start_time": "2025-11-30T09:32:57.631368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Single cell: safe to run on a fresh environment\n",
    "from src.load_data import ensure_data_available\n",
    "\n",
    "# ‚úÖ Creates folders if missing and downloads only if needed\n",
    "ensure_data_available(config=config)\n",
    "\n",
    "print(\"üöÄ Dataset ready\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.virtualenvs/rag-qa/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Dataset already downloaded ‚Äî skipping.\n",
      "üöÄ Dataset ready\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T09:33:05.210392Z",
     "start_time": "2025-11-30T09:33:02.204095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Single cell to explore dataset shards\n",
    "from src.explore_data import load_shards, explore_dataset\n",
    "\n",
    "# Load first few shards to save memory\n",
    "train_ds = load_shards(config.TRAIN_DIR, max_shards=3)\n",
    "val_ds   = load_shards(config.VAL_DIR, max_shards=3)\n",
    "test_ds  = load_shards(config.TEST_DIR, max_shards=3)\n",
    "\n",
    "# Explore datasets\n",
    "explore_dataset(train_ds, \"Train set\")\n",
    "explore_dataset(val_ds, \"Validation set\")\n",
    "explore_dataset(test_ds, \"Test set\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring Train set:\n",
      "Total examples across all shards: 3000\n",
      "Columns: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer']\n",
      "\n",
      "Column types:\n",
      " - question: Value('string')\n",
      " - question_id: Value('string')\n",
      " - question_source: Value('string')\n",
      " - entity_pages: {'doc_source': List(Value('string')), 'filename': List(Value('string')), 'title': List(Value('string')), 'wiki_context': List(Value('string'))}\n",
      " - search_results: {'description': List(Value('null')), 'filename': List(Value('null')), 'rank': List(Value('null')), 'search_context': List(Value('null')), 'title': List(Value('null')), 'url': List(Value('null'))}\n",
      " - answer: {'aliases': List(Value('string')), 'matched_wiki_entity_name': Value('string'), 'normalized_aliases': List(Value('string')), 'normalized_matched_wiki_entity_name': Value('string'), 'normalized_value': Value('string'), 'type': Value('string'), 'value': Value('string')}\n",
      "\n",
      "Sample data from first 3 examples (strings truncated to 50 chars):\n",
      "{'question': 'Which Mediterranean island was once known as Alash...', 'question_id': 'qb_6731', 'question_source': 'http://www.quizballs.com/', 'entity_pages': {'doc_source': ['TagMe', 'TagMe'], 'filename': ['Mediterranean_Sea.txt', 'Alashiya.txt'], 'title': ['Mediterranean Sea', 'Alashiya'], 'wiki_context': ['The\\xa0Mediterranean Sea (pronounced) is a sea  conne...', 'Alashiya, also spelled Alasiya, was a state which ...']}, 'search_results': {'description': [], 'filename': [], 'rank': [], 'search_context': [], 'title': [], 'url': []}, 'answer': {'aliases': ['Culture of Cyprus', 'Kƒ±brƒ±s', 'Etymology of Cyprus', 'History of ancient Cyprus', 'Island of Cyprus', 'Name of Northern Cyprus', 'ISO 3166-1:CY', 'Zypern', 'South Cyprus (Greek Cyprus)', 'Architecture of Cyprus', 'Colony of Cyprus', 'Country CYP', 'Kibris', 'Southern Cyprus', 'Political system of cyprus', 'Greek Cyprus', 'Kypros', 'ŒöœçœÄœÅŒøœÇ', 'Cyrpus', 'Greek Cypriot Administration of Southern Cyprus', 'Republic of Cyprus', 'Ciprus', 'Name of Cyprus', 'ŒöœÖœÄœÅŒπŒ±Œ∫ŒÆ ŒîŒ∑ŒºŒøŒ∫œÅŒ±œÑŒØŒ±', 'Cyprus', 'Cyprus goods', 'Cyprus (Republic of)', 'Greek Republic of Cyprus'], 'matched_wiki_entity_name': '', 'normalized_aliases': ['culture of cyprus', 'kibris', 'political system of cyprus', 'architecture of cyprus', 'south cyprus greek cyprus', 'cyprus', 'kƒ±brƒ±s', 'country cyp', 'greek cyprus', 'cyprus goods', 'greek republic of cyprus', 'greek cypriot administration of southern cyprus', 'cyprus republic of', 'colony of cyprus', 'island of cyprus', 'southern cyprus', 'ciprus', 'name of northern cyprus', 'kypros', 'history of ancient cyprus', 'Œ∫œçœÄœÅŒøœÇ', 'republic of cyprus', 'name of cyprus', 'Œ∫œÖœÄœÅŒπŒ±Œ∫ŒÆ Œ¥Œ∑ŒºŒøŒ∫œÅŒ±œÑŒØŒ±', 'zypern', 'cyrpus', 'etymology of cyprus', 'iso 3166 1 cy'], 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'cyprus', 'type': 'WikipediaEntity', 'value': 'Cyprus'}}\n",
      "{'question': 'What is the top prize at the Cannes Film Festival?', 'question_id': 'qb_6734', 'question_source': 'http://www.quizballs.com/', 'entity_pages': {'doc_source': ['TagMe'], 'filename': ['Cannes_Film_Festival.txt'], 'title': ['Cannes Film Festival'], 'wiki_context': ['The Cannes Festival (French: Festival de Cannes), ...']}, 'search_results': {'description': [], 'filename': [], 'rank': [], 'search_context': [], 'title': [], 'url': []}, 'answer': {'aliases': [\"Palm d'Or\", 'Golden Palm Award', \"The Palme d'Or\", 'Golden palm award', 'The Golden Palm Award', 'Golden Palm Awards', 'Palme d‚ÄôOr', 'Palme d¬¥Or', \"Palme D'or\", \"Palme d'Or\", 'Palm d‚ÄôOr', \"Palme d'or\", \"Palme D'Or\", \"The Palme D'or\", 'Golden Palm'], 'matched_wiki_entity_name': '', 'normalized_aliases': ['golden palm', 'palme d or', 'golden palm awards', 'palm d or', 'golden palm award'], 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'palm d or', 'type': 'WikipediaEntity', 'value': 'Palm d‚ÄôOr'}}\n",
      "{'question': 'The medical condition glaucoma affects which part ...', 'question_id': 'qb_6735', 'question_source': 'http://www.quizballs.com/', 'entity_pages': {'doc_source': ['TagMe', 'TagMe', 'TagMe'], 'filename': ['Disease.txt', 'Glaucoma.txt', 'Human_body.txt'], 'title': ['Disease', 'Glaucoma', 'Human body'], 'wiki_context': ['A disease  is a particular abnormal condition, a d...', 'Glaucoma is a group of eye diseases which result i...', 'The human body is the entire structure of a human ...']}, 'search_results': {'description': [], 'filename': [], 'rank': [], 'search_context': [], 'title': [], 'url': []}, 'answer': {'aliases': ['Eye (anatomy)', 'Eye', 'Eye balls', 'Schizochroal eye', 'Ocular globe', 'Ommateum', 'Simple eye', 'Oculars', 'Animal eyes', 'Eyes', 'Compound Eyes', 'Apposition eye', 'Robotic eye', 'Eye ball', 'Facet eyes', 'Compound Eye', 'Conjunctival disorders', 'Compound eyes', 'Eyeball', 'Cyber-eye', 'Eye (vertebrate)', 'Eye (invertebrate)', 'Ommotidium', \"Fly's eye lens\", 'Peeper (organ)', 'Camera-type eye', 'Ocular', 'Compound eye', 'Eye membrane', 'Pinhole eye'], 'matched_wiki_entity_name': '', 'normalized_aliases': ['compound eyes', 'oculars', 'facet eyes', 'cyber eye', 'ommateum', 'peeper organ', 'eye balls', 'compound eye', 'simple eye', 'eye vertebrate', 'animal eyes', 'camera type eye', 'eye membrane', 'apposition eye', 'eyeball', 'eye', 'ommotidium', 'eyes', 'robotic eye', 'ocular globe', 'eye anatomy', 'pinhole eye', 'eye ball', 'conjunctival disorders', 'fly s eye lens', 'ocular', 'schizochroal eye', 'eye invertebrate'], 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'eye', 'type': 'WikipediaEntity', 'value': 'Eye'}}\n",
      "\n",
      "Basic text statistics (lengths in words) for columns:\n",
      "Column 'question': mean=13.22, min=5, max=46\n",
      "Column 'question_id': mean=1.00, min=1, max=1\n",
      "Column 'question_source': mean=1.00, min=1, max=1\n",
      "\n",
      "Top 10 most common labels in 'answer':\n",
      "[('canada', 14), ('fish', 11), ('australia', 11), ('four', 10), ('red', 10), ('india', 10), ('france', 9), ('three', 9), ('two', 9), ('blue', 9)]\n",
      "\n",
      "Exploring Validation set:\n",
      "Total examples across all shards: 3000\n",
      "Columns: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer']\n",
      "\n",
      "Column types:\n",
      " - question: Value('string')\n",
      " - question_id: Value('string')\n",
      " - question_source: Value('string')\n",
      " - entity_pages: {'doc_source': List(Value('string')), 'filename': List(Value('string')), 'title': List(Value('string')), 'wiki_context': List(Value('string'))}\n",
      " - search_results: {'description': List(Value('null')), 'filename': List(Value('null')), 'rank': List(Value('null')), 'search_context': List(Value('null')), 'title': List(Value('null')), 'url': List(Value('null'))}\n",
      " - answer: {'aliases': List(Value('string')), 'matched_wiki_entity_name': Value('string'), 'normalized_aliases': List(Value('string')), 'normalized_matched_wiki_entity_name': Value('string'), 'normalized_value': Value('string'), 'type': Value('string'), 'value': Value('string')}\n",
      "\n",
      "Sample data from first 3 examples (strings truncated to 50 chars):\n",
      "{'question': 'Where in England was Dame Judi Dench born?', 'question_id': 'tc_3', 'question_source': 'http://www.triviacountry.com/', 'entity_pages': {'doc_source': ['TagMe', 'TagMe'], 'filename': ['England.txt', 'Judi_Dench.txt'], 'title': ['England', 'Judi Dench'], 'wiki_context': ['England is a country that is part of the United Ki...', 'Dame Judith Olivia \"Judi\" Dench,  (born 9 December...']}, 'search_results': {'description': [], 'filename': [], 'rank': [], 'search_context': [], 'title': [], 'url': []}, 'answer': {'aliases': ['Park Grove (1895)', 'York UA', 'Yorkish', 'UN/LOCODE:GBYRK', 'York, UK', 'Eoforwic', 'Park Grove School', 'York Ham', 'The weather in York', 'City of York', 'York, England', 'York, Yorkshire', 'York ham', 'County Borough of York', 'YORK', 'Eoferwic', 'Park Grove Primary School', 'York, North Yorkshire', 'Yoisk', 'York', 'York (England)'], 'matched_wiki_entity_name': '', 'normalized_aliases': ['york yorkshire', 'eoferwic', 'park grove primary school', 'park grove school', 'weather in york', 'park grove 1895', 'eoforwic', 'county borough of york', 'york uk', 'un locode gbyrk', 'city of york', 'york england', 'york ua', 'york ham', 'york', 'yorkish', 'yoisk', 'york north yorkshire'], 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'york', 'type': 'WikipediaEntity', 'value': 'York'}}\n",
      "{'question': 'From which country did Angola achieve independence...', 'question_id': 'tc_8', 'question_source': 'http://www.triviacountry.com/', 'entity_pages': {'doc_source': ['TagMe', 'TagMe', 'Search'], 'filename': ['Nation_state.txt', 'Angola.txt', 'Angolan_Civil_War.txt'], 'title': ['Nation state', 'Angola', 'Angolan Civil War'], 'wiki_context': ['A nation state is a type of state that conjoins th...', 'Angola, officially the Republic of Angola (; Kikon...', 'The Angolan Civil War () was a major civil conflic...']}, 'search_results': {'description': [], 'filename': [], 'rank': [], 'search_context': [], 'title': [], 'url': []}, 'answer': {'aliases': ['Portoga≈Ço', 'Republic of Portugal', 'PORTUGAL', 'Portekiz', 'Portugallu', 'O Papagaio', 'ISO 3166-1:PT', 'Portunga', 'Phu-to-ga', 'Potigal', 'Port√ªnga', 'Portugul', 'An Phortaing√©il', 'PortugƒÅle', 'Portugale', 'Portingale', 'Potiti', 'Portugali', 'Portugall', 'Portek√Æz', 'Bo Dao Nha', 'Portuguese Republic', 'Portogallo', 'Portugaul', 'Portogalo', 'Portyngal', 'Yn Phortiugal', 'Portugalio', 'Portug√°l', 'Portugual', 'Portuga', 'Portgual', 'Portugalsko', 'Portugaleje', 'Ph√ª-t√¥-g√¢', 'Portugalujo', 'Portugalija', 'Pertual', 'P√≤tigal', 'Portugal', 'B·ªì ƒê√†o Nha', 'Portugalska', 'Rep√∫blica Portuguesa', 'Portiwgal', 'Portugalƒójƒó', 'Port√∫gal', 'Portegal', 'An Phortaingeil', 'Republica Portuguesa'], 'matched_wiki_entity_name': '', 'normalized_aliases': ['portugul', 'portugallu', 'portugalska', 'p√≤tigal', 'portugaul', 'portugalujo', 'portuguese republic', 'iso 3166 1 pt', 'republic of portugal', 'portugalsko', 'portugual', 'b·ªì ƒë√†o nha', 'portugall', 'port√ªnga', 'bo dao nha', 'phortaingeil', 'portugale', 'portugal', 'portug√°l', 'portugalƒójƒó', 'portiwgal', 'phu to ga', 'portugalija', 'portugalio', 'portogallo', 'ph√ª t√¥ g√¢', 'portegal', 'rep√∫blica portuguesa', 'portugƒÅle', 'phortaing√©il', 'yn phortiugal', 'portoga≈Ço', 'portuga', 'portugaleje', 'portekiz', 'o papagaio', 'portunga', 'potigal', 'portek√Æz', 'pertual', 'portogalo', 'portugali', 'portyngal', 'republica portuguesa', 'portingale', 'port√∫gal', 'portgual', 'potiti'], 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'portugal', 'type': 'WikipediaEntity', 'value': 'Portugal'}}\n",
      "{'question': 'Which city does David Soul come from?', 'question_id': 'tc_9', 'question_source': 'http://www.triviacountry.com/', 'entity_pages': {'doc_source': ['TagMe'], 'filename': ['David_Soul.txt'], 'title': ['David Soul'], 'wiki_context': ['David Soul (born August 28, 1943) is an American-B...']}, 'search_results': {'description': [], 'filename': [], 'rank': [], 'search_context': [], 'title': [], 'url': []}, 'answer': {'aliases': ['Chi-Beria', 'Sayre language academy', 'Chicago', 'Chicago, Illinois', 'Hog Butcher for the World', 'Land of smelly onions', 'Ariel Community Academy', 'The weather in Chicago', 'Chicago, Illinois, U.S.A.', 'Chicago, Illionis', 'Near North Montessori', 'Religion in Chicago', 'Chicago Finance Committee', 'The Paris of America', 'The city of Chicago', 'City of Chicago', 'List of sister cities of Chicago', 'UN/LOCODE:USCHI', 'Chicago theatre scene', 'Chicago, WI', 'The City of Broad Shoulders', 'City of Broad Shoulders', 'Sister Cities of Chicago', 'Chicago il', 'Chicago, Illinois, USA', 'Performing arts in Chicago', 'Chicago Transportation Committee', 'Chicago, Wisconsin', 'City of chicago', 'Chicago theater scene', 'Chicago, Il', 'Chicago, IL.', 'Chicago, Ill.', 'City of Chicago, Illinois', 'Chi town', 'Chicago, United States', 'Chicago (Ill.)', 'Transport in Chicago', 'Chicago, Illinois, United States', 'Chicago (IL)', 'USCHI', 'Chichago', 'Chcago', 'Chicago, Illinois, U.S.', 'Sister Cities Chicago', 'Chicago, USA', 'Chi City', 'Chicago, IL', 'Chi-Town', 'Chicago theatre', 'Paris of America', 'Chicago, Illinois, US', 'Chicago Illinois', 'The city of Chicago, Illinois', 'Sister cities of Chicago'], 'matched_wiki_entity_name': '', 'normalized_aliases': ['sayre language academy', 'chicago transportation committee', 'chicago illinois u s', 'sister cities of chicago', 'sister cities chicago', 'transport in chicago', 'chicago illinois', 'chicago illinois usa', 'chi town', 'hog butcher for world', 'religion in chicago', 'chicago', 'chicago wi', 'near north montessori', 'un locode uschi', 'city of broad shoulders', 'chicago theatre', 'chicago usa', 'uschi', 'chicago il', 'city of chicago', 'chicago finance committee', 'list of sister cities of chicago', 'chi beria', 'weather in chicago', 'chicago wisconsin', 'land of smelly onions', 'ariel community academy', 'chicago theater scene', 'chicago united states', 'paris of america', 'chicago illionis', 'chicago illinois united states', 'chcago', 'chi city', 'chicago illinois us', 'performing arts in chicago', 'chicago theatre scene', 'chichago', 'chicago ill', 'city of chicago illinois'], 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'chicago', 'type': 'WikipediaEntity', 'value': 'Chicago'}}\n",
      "\n",
      "Basic text statistics (lengths in words) for columns:\n",
      "Column 'question': mean=11.86, min=5, max=43\n",
      "Column 'question_id': mean=1.00, min=1, max=1\n",
      "Column 'question_source': mean=1.00, min=1, max=1\n",
      "\n",
      "Top 10 most common labels in 'answer':\n",
      "[('three', 8), ('ireland', 8), ('spain', 8), ('portugal', 7), ('norway', 6), ('italy', 6), ('6', 6), ('two', 6), ('chicago', 5), ('switzerland', 5)]\n",
      "\n",
      "Exploring Test set:\n",
      "Total examples across all shards: 3000\n",
      "Columns: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer']\n",
      "\n",
      "Column types:\n",
      " - question: Value('string')\n",
      " - question_id: Value('string')\n",
      " - question_source: Value('string')\n",
      " - entity_pages: {'doc_source': List(Value('string')), 'filename': List(Value('string')), 'title': List(Value('string')), 'wiki_context': List(Value('string'))}\n",
      " - search_results: {'description': List(Value('null')), 'filename': List(Value('null')), 'rank': List(Value('null')), 'search_context': List(Value('null')), 'title': List(Value('null')), 'url': List(Value('null'))}\n",
      " - answer: {'aliases': List(Value('string')), 'matched_wiki_entity_name': Value('string'), 'normalized_aliases': List(Value('string')), 'normalized_matched_wiki_entity_name': Value('string'), 'normalized_value': Value('string'), 'type': Value('string'), 'value': Value('string')}\n",
      "\n",
      "Sample data from first 3 examples (strings truncated to 50 chars):\n",
      "{'question': 'Which Lloyd Webber musical premiered in the US on ...', 'question_id': 'tc_33', 'question_source': 'http://www.triviacountry.com/', 'entity_pages': {'doc_source': ['TagMe'], 'filename': ['Andrew_Lloyd_Webber.txt'], 'title': ['Andrew Lloyd Webber'], 'wiki_context': ['Andrew Lloyd Webber, Baron Lloyd-Webber   (born 22...']}, 'search_results': {'description': [], 'filename': [], 'rank': [], 'search_context': [], 'title': [], 'url': []}, 'answer': {'aliases': ['Sunset Blvd', 'West Sunset Boulevard', 'Sunset Boulevard', 'Sunset Bulevard', 'Sunset Blvd.'], 'matched_wiki_entity_name': '', 'normalized_aliases': ['sunset boulevard', 'sunset bulevard', 'west sunset boulevard', 'sunset blvd'], 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'sunset boulevard', 'type': 'WikipediaEntity', 'value': 'Sunset Boulevard'}}\n",
      "{'question': 'Who was the next British Prime Minister after Arth...', 'question_id': 'tc_40', 'question_source': 'http://www.triviacountry.com/', 'entity_pages': {'doc_source': ['TagMe', 'TagMe'], 'filename': ['Prime_Minister_of_the_United_Kingdom.txt', 'Arthur_Balfour.txt'], 'title': ['Prime Minister of the United Kingdom', 'Arthur Balfour'], 'wiki_context': ['The Prime Minister of the United Kingdom of Great ...', 'Arthur James Balfour, 1st Earl of Balfour,  (;  25...']}, 'search_results': {'description': [], 'filename': [], 'rank': [], 'search_context': [], 'title': [], 'url': []}, 'answer': {'aliases': ['Sir Henry Campbell-Bannerman', 'Campbell-Bannerman', 'Campbell Bannerman', 'Sir Henry Campbell Bannerman', 'Henry Campbell Bannerman', 'Henry Campbell-Bannerman'], 'matched_wiki_entity_name': '', 'normalized_aliases': ['henry campbell bannerman', 'sir henry campbell bannerman', 'campbell bannerman'], 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'campbell bannerman', 'type': 'WikipediaEntity', 'value': 'Campbell-Bannerman'}}\n",
      "{'question': 'Who had a 70s No 1 hit with Kiss You All Over?', 'question_id': 'tc_49', 'question_source': 'http://www.triviacountry.com/', 'entity_pages': {'doc_source': ['TagMe'], 'filename': ['Kiss_You_All_Over.txt'], 'title': ['Kiss You All Over'], 'wiki_context': ['\"Kiss You All Over\" is a 1978 song performed by th...']}, 'search_results': {'description': [], 'filename': [], 'rank': [], 'search_context': [], 'title': [], 'url': []}, 'answer': {'aliases': ['Internal exile', 'Exiles', 'Transported for life', 'Exile (politics and government)', 'Voluntary exile', 'Sent into exile', 'Exile and Banishment', 'Self-exile', 'Forced exile', 'Exile', 'Exile in Greek tragedy', 'Banish', 'Banishment'], 'matched_wiki_entity_name': '', 'normalized_aliases': ['exiles', 'voluntary exile', 'forced exile', 'banish', 'self exile', 'exile politics and government', 'exile in greek tragedy', 'sent into exile', 'banishment', 'transported for life', 'exile', 'internal exile', 'exile and banishment'], 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'exile', 'type': 'WikipediaEntity', 'value': 'Exile'}}\n",
      "\n",
      "Basic text statistics (lengths in words) for columns:\n",
      "Column 'question': mean=13.32, min=5, max=55\n",
      "Column 'question_id': mean=1.00, min=1, max=1\n",
      "Column 'question_source': mean=1.00, min=1, max=1\n",
      "\n",
      "Top 10 most common labels in 'answer':\n",
      "[('france', 12), ('argentina', 9), ('three', 8), ('red', 8), ('australia', 8), ('spain', 8), ('new zealand', 7), ('switzerland', 7), ('scotland', 7), ('blue', 7)]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T09:33:09.304376Z",
     "start_time": "2025-11-30T09:33:07.903106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.analyze_data import load_shards_concat, dataset_info, analyze_lengths, most_common_answers, print_sample_qa\n",
    "\n",
    "# Load datasets\n",
    "train_ds = load_shards_concat(config.TRAIN_DIR)\n",
    "val_ds   = load_shards_concat(config.VAL_DIR)\n",
    "test_ds  = load_shards_concat(config.TEST_DIR)\n",
    "\n",
    "# Explore datasets and save plots in the 'plots/' folder\n",
    "# for name, ds in [(\"Train\", train_ds), (\"Validation\", val_ds), (\"Test\", test_ds)]:\n",
    "#     if ds is None:\n",
    "#         print(f\"No dataset found for {name}\")\n",
    "#         continue\n",
    "#     dataset_info(ds, name)\n",
    "#     analyze_lengths(ds, \"question\", name)\n",
    "#     analyze_lengths(ds, \"answer\", name)\n",
    "#     most_common_answers(ds)\n",
    "#     print_sample_qa(ds, name, n=5)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T09:33:37.106432Z",
     "start_time": "2025-11-30T09:33:14.258647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.compute_embeddings import compute_embeddings, retrieve_top_k\n",
    "\n",
    "# Compute embeddings (will load from file if already exists)\n",
    "corpus, corpus_embeddings = compute_embeddings(config=config)\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is the capital of france?\"\n",
    "results, scores = retrieve_top_k(query=query, corpus=corpus, corpus_embeddings=corpus_embeddings, config=config, top_k=3)\n",
    "\n",
    "print(\"\\nTop 3 retrieved passages for query:\")\n",
    "for passage, score in zip(results, scores):\n",
    "    print(f\"[score: {score:.4f}] {passage}\\n---\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved embeddings from /mnt/c/dev/ml/rag-qa/corpus_embeddings_unique.pkl...\n",
      "Loaded 978526 passages.\n",
      "\n",
      "Top 3 retrieved passages for query:\n",
      "[score: 0.8731] Paris: paris ( french : ) is the capital and most populous city of france. situated on the river seine in northern metropolitan france, it is in the centre of the ile - de - france region, also known as the region parisienne, \" paris region \". the commune of paris largely retains its one and a half century old administrative boundaries, with an area of 105 km¬≤ ( 41 mi¬≤ ) and a population of 2, 241, 346. together with its suburbs, the whole agglomeration has a population of 10, 550, 350 ( jan. 2012 census ). paris'metropolitan area spans most of the ile - de - france region and has a population of 12, 405, 426 ( jan. 2013 census ), constituting one - fifth of the population of france. the administrative region covers 12, 012 km¬≤ ( 4, 638 mi¬≤ ), with approximately 12 million inhabitants, and has its own regional council and president. paris was founded in the 3rd century bc by a celtic people called the parisii, who gave the city its name. by the 12th century, paris was the largest city in the western world, a prosperous trading centre, and the\n",
      "---\n",
      "[score: 0.8564] Capital city: valparaiso. * : prague is the sole constitutional capital. brno is home to all three of the country's highest courts, making it the de facto capital of the czech judicial branch. * : the supreme court and the ministry of education and research are located in tartu. * : during the summer, the president resides at the kultaranta in naantali ; presidential sessions of the government are held there as well. * : the french constitution does not recognise any capital city in france. by law paris is the seat of both houses of parliament ( the national assembly and the senate ), but their joint congresses are held at the palace of versailles. in case of emergency, the seat of the constitutional powers can be transferred in another town, in order for the houses of parliament to keep seating in the same location of the president and cabinet. * : the official capital berlin is home to the parliament and the highest bodies of the executive branch ( consisting of the ceremonial presidency and effective chancellery ). various ministries are located in the former west german capital of bonn, which now has the title federal city. the federal constitutional court has its seat in karlsruhe, which as a consequence is\n",
      "---\n",
      "[score: 0.8501] France: france ( french : ), officially the french republic ( ), is a sovereign state comprising territory in western europe and several overseas regions and territories. the european, or metropolitan, area of france extends from the mediterranean sea to the english channel and the north sea, and from the rhine to the atlantic ocean. france spans 643801 km2 and has a total population of 66. 7 million. it is a unitary semi - presidential republic with the capital in paris, the country's largest city and main cultural and commercial centre. during the iron age, what is now metropolitan france was inhabited by the gauls, a celtic people. the area was annexed in 51 bc by rome, which held gaul until 486, when the germanic franks conquered the region and formed the kingdom of france. france emerged as a major european power in the late middle ages, with its victory in the hundred years'war ( 1337 to 1453 ) strengthening state - building and political centralization. during the renaissance, french culture flourished and a global colonial empire was established, which by the 20th century would be the second largest in the world. the 16th century was dominated by religious civil wars between catholics and protestants ( hug\n",
      "---\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.generator import load_embeddings, generate_answer_combined\n",
    "\n",
    "corpus, emb = load_embeddings()\n",
    "\n",
    "query = \"What is the capital of france?\"\n",
    "answer, ctx = generate_answer_combined(query, corpus, emb, top_k=5)\n",
    "\n",
    "print(\"\\nüîç Used Context Passages:\\n\")\n",
    "for i,p in enumerate(ctx,1):\n",
    "    print(f\"{i}. {p[:200].replace(chr(10),' ')}...\\n\")\n",
    "\n",
    "print(\"üí° Final Answer:\\n\", answer)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate ONLY RETRIEVE Performance"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T09:41:37.062549Z",
     "start_time": "2025-11-30T09:33:44.754860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.evaluate_retrieve import run_evaluation\n",
    "\n",
    "run_evaluation(config=config)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loaded FAISS index with 978526 passages\n",
      "BAAI/bge-base-en\n",
      "\n",
      "=== üî• Evaluating TRAIN ‚Äî first 1000 samples ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /mnt/c/dev/ml/rag-qa/data/train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54/54 [00:00<00:00, 140.49it/s]\n",
      "Evaluating Recall: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:27<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.8130\n",
      "Recall@3: 0.9050\n",
      "Recall@5: 0.9260\n",
      "Recall@7: 0.9390\n",
      "Recall@10: 0.9410\n",
      "\n",
      "=== üî• Evaluating VALIDATION ‚Äî first 1000 samples ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /mnt/c/dev/ml/rag-qa/data/validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 126.32it/s]\n",
      "Evaluating Recall: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:27<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.7260\n",
      "Recall@3: 0.8320\n",
      "Recall@5: 0.8580\n",
      "Recall@7: 0.8700\n",
      "Recall@10: 0.8820\n",
      "\n",
      "=== üî• Evaluating TEST ‚Äî first 1000 samples ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /mnt/c/dev/ml/rag-qa/data/test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 131.04it/s]\n",
      "Evaluating Recall: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [02:27<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.7820\n",
      "Recall@3: 0.8790\n",
      "Recall@5: 0.9030\n",
      "Recall@7: 0.9140\n",
      "Recall@10: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.evaluate_rag_full import run_full_rag_eval\n",
    "\n",
    "run_full_rag_eval(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
