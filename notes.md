# Poster ideas

comare with or without tokenizer

top k retriever comparison


compare 2 llms at generator part


visualize pipeline
concrete exampel with question, retrieved chunks, generated answer, expected answer.

evaluation of whole pipeline

compare acc on test and train dataset

# Future ideas

- Llama 3.1 (run locally)
- few shot prompt
- try out sparse embedding
- clean up code & repo

# Open questions

- How to evaluate?
- Why FAISS?
- How does F1 evaluation work?
- How does the re-ranker work (similarity/quality/relevance/...?)
- embed all dataset splits or only a few?
- Are train and test dataset disjoint?
- wich LaTeX formatting? IEEE?

# Poster improvements

- grouped bar chart
- example question (maybe in pipeline)
- add chunking size 
- (dataset description)
- what to do next
- enhace motivation part
- re-ranking in extra box in pipeline